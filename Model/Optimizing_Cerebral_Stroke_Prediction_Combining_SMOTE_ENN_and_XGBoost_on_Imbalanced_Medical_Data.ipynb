{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcPrrLhkA5A5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
        "from imblearn.combine import SMOTEENN\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Step 1: Load Dataset\n",
        "# Replace with actual dataset path\n",
        "data = pd.read_csv(\"stroke_dataset.csv\")\n",
        "\n",
        "# Step 2: Handle Missing Values\n",
        "# Impute missing BMI with median\n",
        "data['bmi'].fillna(data['bmi'].median(), inplace=True)\n",
        "\n",
        "# Impute smoking status with mode or predictive imputation\n",
        "data['smoking_status'].fillna(data['smoking_status'].mode()[0], inplace=True)\n",
        "\n",
        "# Step 3: Preprocessing\n",
        "# Drop irrelevant columns\n",
        "data.drop(['Patient_ID', 'Residence_type'], axis=1, inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "features = [col for col in data.columns if col != 'stroke']\n",
        "data[features] = scaler.fit_transform(data[features])\n",
        "\n",
        "# Step 4: Handle Class Imbalance\n",
        "X = data.drop('stroke', axis=1)\n",
        "y = data['stroke']\n",
        "\n",
        "smote_enn = SMOTEENN(random_state=42)\n",
        "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
        "\n",
        "# Step 5: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train XGBoost with Hyperparameter Tuning\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(xgb, param_grid, cv=3, scoring='roc_auc', n_iter=10, random_state=42)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "\n",
        "# Step 7: Evaluate the Model\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nAUC-ROC Score:\")\n",
        "print(roc_auc_score(y_test, y_prob))\n",
        "\n",
        "# Optional: Save the model\n",
        "import joblib\n",
        "joblib.dump(best_model, \"stroke_prediction_model.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "# Step 1: Load Dataset\n",
        "data = pd.read_csv(\"stroke_dataset.csv\")\n",
        "\n",
        "# Step 2: Handle Missing Values\n",
        "data['bmi'].fillna(data['bmi'].median(), inplace=True)\n",
        "data['smoking_status'].fillna(data['smoking_status'].mode()[0], inplace=True)\n",
        "\n",
        "# Drop irrelevant columns and preprocess data\n",
        "data.drop(['Patient_ID', 'Residence_type'], axis=1, inplace=True)\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "scaler = StandardScaler()\n",
        "features = [col for col in data.columns if col != 'stroke']\n",
        "data[features] = scaler.fit_transform(data[features])\n",
        "\n",
        "X = data.drop('stroke', axis=1)\n",
        "y = data['stroke']\n",
        "\n",
        "# Step 3: Define Models\n",
        "models = {\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"SVM\": SVC(probability=True),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "}\n",
        "\n",
        "# Step 4: Define Sampling Techniques\n",
        "sampling_techniques = {\n",
        "    \"SMOTE\": SMOTE(random_state=42),\n",
        "    \"Borderline SMOTE\": BorderlineSMOTE(random_state=42),\n",
        "    \"ADASYN\": ADASYN(random_state=42),\n",
        "    \"SMOTE-ENN\": SMOTEENN(random_state=42),\n",
        "    \"Tomek Links\": TomekLinks()\n",
        "}\n",
        "\n",
        "# Step 5: Evaluate Models with Each Sampling Technique\n",
        "results = []\n",
        "for sampling_name, sampler in sampling_techniques.items():\n",
        "    print(f\"\\n=== Sampling Technique: {sampling_name} ===\")\n",
        "    if sampling_name == \"Tomek Links\":\n",
        "        # Tomek Links is an under-sampling technique, so apply only after initial sampling\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_resampled, y_resampled)\n",
        "    else:\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\n--- Model: {model_name} ---\")\n",
        "        if model_name == \"XGBoost\":\n",
        "            # Hyperparameter tuning for XGBoost\n",
        "            param_grid = {\n",
        "                'n_estimators': [100, 200],\n",
        "                'max_depth': [3, 5],\n",
        "                'learning_rate': [0.01, 0.1]\n",
        "            }\n",
        "            search = RandomizedSearchCV(model, param_grid, cv=3, scoring='roc_auc', n_iter=5, random_state=42)\n",
        "            search.fit(X_train, y_train)\n",
        "            model = search.best_estimator_\n",
        "\n",
        "        # Train model\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "        # Evaluate\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        auc = roc_auc_score(y_test, y_prob) if y_prob is not None else \"N/A\"\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "        results.append({\n",
        "            \"Sampling\": sampling_name,\n",
        "            \"Model\": model_name,\n",
        "            \"Accuracy\": accuracy,\n",
        "            \"AUC\": auc,\n",
        "            \"Sensitivity\": report['1']['recall'],\n",
        "            \"Specificity\": report['0']['recall'],\n",
        "            \"F1-Score\": report['1']['f1-score'],\n",
        "            \"Confusion Matrix\": cm\n",
        "        })\n",
        "\n",
        "# Step 6: Display Results\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n=== Results Summary ===\")\n",
        "print(results_df)\n",
        "\n",
        "# Optional: Save results\n",
        "results_df.to_csv(\"model_comparison_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "XyYD0mTGBivp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}